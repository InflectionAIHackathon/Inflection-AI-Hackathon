#!/usr/bin/env python3
"""
Enhance Training Data Quality
=============================

This script addresses the data quality issues causing poor R¬≤ scores:
1. Remove unrealistic yield outliers
2. Impute missing values intelligently
3. Add derived features for better predictions
4. Validate data consistency across counties
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataQualityEnhancer:
    """Enhance training data quality to improve model performance"""
    
    def __init__(self):
        self.enhanced_data = None
        self.data_quality_report = {}
        
    def load_and_analyze_data(self, data_path="data/master_water_scarcity_dataset.csv"):
        """Load data and perform initial analysis"""
        logger.info("üìä Loading and analyzing training data...")
        
        # Load data
        df = pd.read_csv(data_path)
        logger.info(f"‚úÖ Raw data loaded: {len(df):,} records")
        
        # Initial data quality assessment
        self._assess_data_quality(df)
        
        return df
    
    def _assess_data_quality(self, df):
        """Assess data quality issues"""
        logger.info("üîç Assessing data quality...")
        
        # Check for missing values
        missing_data = df.isnull().sum()
        logger.info(f"Missing values per column:")
        for col, missing in missing_data.items():
            if missing > 0:
                logger.info(f"  {col}: {missing} ({missing/len(df)*100:.1f}%)")
        
        # Check for unrealistic values
        yield_col = 'Maize_Yield_tonnes_ha'
        if yield_col in df.columns:
            yield_stats = df[yield_col].describe()
            logger.info(f"\nYield statistics:")
            logger.info(f"  Min: {yield_stats['min']:.3f} t/ha")
            logger.info(f"  Max: {yield_stats['max']:.3f} t/ha")
            logger.info(f"  Mean: {yield_stats['mean']:.3f} t/ha")
            logger.info(f"  Std: {yield_stats['std']:.3f} t/ha")
            
            # Identify outliers
            q1 = yield_stats['25%']
            q3 = yield_stats['75%']
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            outliers = df[(df[yield_col] < lower_bound) | (df[yield_col] > upper_bound)]
            logger.info(f"  Outliers (IQR method): {len(outliers)} records")
            
            # Check for suspicious values
            suspicious_low = df[df[yield_col] < 0.5]
            suspicious_high = df[df[yield_col] > 5.0]
            logger.info(f"  Suspicious low (<0.5): {len(suspicious_low)} records")
            logger.info(f"  Suspicious high (>5.0): {len(suspicious_high)} records")
    
    def create_enhanced_dataset(self, df):
        """Create enhanced dataset with better features"""
        logger.info("üöÄ Creating enhanced dataset...")
        
        # 1. Create annual aggregated dataset
        logger.info("Creating annual aggregated dataset...")
        
        annual_data = df.groupby(['County', 'Year']).agg({
            'Monthly_Precipitation_mm': ['sum', 'mean', 'std'],  # Rainfall patterns
            'Monthly_Temperature_C': ['mean', 'std'],            # Temperature patterns
            'Monthly_Humidity_Percent': ['mean', 'std'],         # Humidity patterns
            'Soil_pH_H2O': 'mean',                              # Average soil pH
            'Soil_Organic_Carbon': 'mean',                      # Average organic carbon
            'Soil_Clay': 'mean',                                # Average clay content
            'Maize_Yield_tonnes_ha': 'mean'                     # Average yield
        }).reset_index()
        
        # Flatten column names
        annual_data.columns = [
            'County', 'Year',
            'Annual_Rainfall_mm', 'Avg_Rainfall_mm', 'Rainfall_Std_mm',
            'Avg_Temperature_C', 'Temperature_Std_C',
            'Avg_Humidity_Percent', 'Humidity_Std_Percent',
            'Soil_pH', 'Soil_Organic_Carbon', 'Soil_Clay_Content',
            'Maize_Yield_tonnes_ha'
        ]
        
        logger.info(f"‚úÖ Annual dataset created: {len(annual_data):,} records")
        
        # 2. Add derived features
        logger.info("Adding derived features...")
        
        # Growing season length (months with >50mm rainfall)
        annual_data['Growing_Season_Months'] = np.where(
            annual_data['Annual_Rainfall_mm'] > 600,  # Good growing season
            np.where(annual_data['Annual_Rainfall_mm'] > 400,  # Moderate season
                    np.where(annual_data['Annual_Rainfall_mm'] > 200, 2, 1),  # Short season
                    3),  # Long season
            0  # No growing season
        )
        
        # Water stress index (rainfall vs temperature)
        annual_data['Water_Stress_Index'] = (
            annual_data['Annual_Rainfall_mm'] / (annual_data['Avg_Temperature_C'] + 1)
        )
        
        # Soil quality score
        annual_data['Soil_Quality_Score'] = (
            annual_data['Soil_pH'] * 0.3 +
            annual_data['Soil_Organic_Carbon'] * 0.4 +
            annual_data['Soil_Clay_Content'] * 0.3
        )
        
        # Climate variability
        annual_data['Climate_Variability'] = (
            annual_data['Rainfall_Std_mm'] + 
            annual_data['Temperature_Std_C'] + 
            annual_data['Humidity_Std_Percent']
        )
        
        logger.info("‚úÖ Derived features added")
        
        # 3. Clean unrealistic yield data
        logger.info("üßπ Cleaning unrealistic yield data...")
        
        before_clean = len(annual_data)
        
        # Remove extreme outliers using multiple methods
        # Method 1: IQR method
        q1 = annual_data['Maize_Yield_tonnes_ha'].quantile(0.25)
        q3 = annual_data['Maize_Yield_tonnes_ha'].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        # Method 2: Domain knowledge (Kenya realistic yields)
        domain_lower = 0.8  # Minimum realistic yield for Kenya
        domain_upper = 5.0  # Maximum realistic yield for Kenya
        
        # Apply both filters
        annual_data = annual_data[
            (annual_data['Maize_Yield_tonnes_ha'] >= domain_lower) &
            (annual_data['Maize_Yield_tonnes_ha'] <= domain_upper) &
            (annual_data['Maize_Yield_tonnes_ha'] >= lower_bound) &
            (annual_data['Maize_Yield_tonnes_ha'] <= upper_bound)
        ]
        
        after_clean = len(annual_data)
        removed_count = before_clean - after_clean
        
        logger.info(f"‚úÖ Data cleaning: Removed {removed_count} unrealistic yield records")
        logger.info(f"‚úÖ Clean data: {len(annual_data):,} records remaining")
        
        # 4. Impute missing values
        logger.info("üîß Imputing missing values...")
        
        # Check missing values after cleaning
        missing_after_clean = annual_data.isnull().sum()
        if missing_after_clean.sum() > 0:
            logger.info("Missing values after cleaning:")
            for col, missing in missing_after_clean.items():
                if missing > 0:
                    logger.info(f"  {col}: {missing} ({missing/len(annual_data)*100:.1f}%)")
            
            # Impute numerical columns
            numerical_cols = annual_data.select_dtypes(include=[np.number]).columns
            imputer = SimpleImputer(strategy='mean')
            annual_data[numerical_cols] = imputer.fit_transform(annual_data[numerical_cols])
            
            logger.info("‚úÖ Missing values imputed using mean strategy")
        else:
            logger.info("‚úÖ No missing values found")
        
        # 5. Validate county distribution
        county_counts = annual_data['County'].value_counts()
        logger.info(f"\nüèòÔ∏è County Distribution After Enhancement:")
        logger.info(f"  Total counties: {len(county_counts)}")
        logger.info(f"  Records per county: {county_counts.min()} - {county_counts.max()}")
        logger.info(f"  Average records per county: {county_counts.mean():.1f}")
        
        # 6. Final data quality check
        logger.info("\nüìä Final Data Quality Report:")
        for col in ['Maize_Yield_tonnes_ha', 'Annual_Rainfall_mm', 'Soil_pH', 'Soil_Organic_Carbon']:
            if col in annual_data.columns:
                stats = annual_data[col].describe()
                logger.info(f"  {col}:")
                logger.info(f"    Min: {stats['min']:.2f}")
                logger.info(f"    Max: {stats['max']:.2f}")
                logger.info(f"    Mean: {stats['mean']:.2f}")
                logger.info(f"    Std: {stats['std']:.2f}")
        
        self.enhanced_data = annual_data
        return annual_data
    
    def save_enhanced_data(self, output_path="data/enhanced_maize_dataset.csv"):
        """Save the enhanced dataset"""
        if self.enhanced_data is None:
            raise ValueError("No enhanced data to save. Run create_enhanced_dataset first.")
        
        # Create output directory
        output_file = Path(output_path)
        output_file.parent.mkdir(exist_ok=True)
        
        # Save enhanced data
        self.enhanced_data.to_csv(output_file, index=False)
        logger.info(f"‚úÖ Enhanced dataset saved to: {output_file}")
        
        # Save data quality report
        report_file = output_file.parent / "data_quality_report.txt"
        with open(report_file, 'w') as f:
            f.write("Data Quality Enhancement Report\n")
            f.write("=" * 40 + "\n\n")
            f.write(f"Original records: {len(self.enhanced_data)}\n")
            f.write(f"Features: {len(self.enhanced_data.columns)}\n")
            f.write(f"Counties: {len(self.enhanced_data['County'].unique())}\n")
            f.write(f"Years: {len(self.enhanced_data['Year'].unique())}\n\n")
            
            f.write("Feature Summary:\n")
            for col in self.enhanced_data.columns:
                if col not in ['County', 'Year']:
                    f.write(f"  {col}: {self.enhanced_data[col].dtype}\n")
        
        logger.info(f"‚úÖ Data quality report saved to: {report_file}")
        return output_file

def main():
    """Main execution function"""
    logger.info("üöÄ Starting Data Quality Enhancement")
    logger.info("=" * 70)
    
    # Initialize enhancer
    enhancer = DataQualityEnhancer()
    
    try:
        # 1. Load and analyze data
        logger.info("\n" + "="*70)
        raw_data = enhancer.load_and_analyze_data()
        
        # 2. Create enhanced dataset
        logger.info("\n" + "="*70)
        enhanced_data = enhancer.create_enhanced_dataset(raw_data)
        
        # 3. Save enhanced data
        logger.info("\n" + "="*70)
        output_file = enhancer.save_enhanced_data()
        
        # 4. Final summary
        logger.info("\n" + "="*70)
        logger.info("üéØ DATA ENHANCEMENT COMPLETE")
        logger.info("=" * 70)
        logger.info("‚úÖ Data quality issues identified and fixed")
        logger.info("‚úÖ Unrealistic yields removed")
        logger.info("‚úÖ Derived features added")
        logger.info("‚úÖ Missing values imputed")
        logger.info("‚úÖ County distribution validated")
        logger.info(f"‚úÖ Enhanced dataset saved: {output_file}")
        
        logger.info(f"\nüìä Enhanced Dataset Summary:")
        logger.info(f"  Total records: {len(enhanced_data):,}")
        logger.info(f"  Total features: {len(enhanced_data.columns)}")
        logger.info(f"  Counties: {len(enhanced_data['County'].unique())}")
        logger.info(f"  Years: {len(enhanced_data['Year'].unique())}")
        
        logger.info(f"\nüöÄ Next Steps:")
        logger.info("  1. Retrain model with enhanced data")
        logger.info("  2. Expect R¬≤ improvement from 0.7 to >0.85")
        logger.info("  3. Test county-specific predictions")
        
    except Exception as e:
        logger.error(f"‚ùå Error during data enhancement: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
